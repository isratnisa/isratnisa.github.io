<!DOCTYPE html>
<html lang="en">
<head>
<title>CSS Template</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
* {
  box-sizing: border-box;
}

body {
  font-family: Arial, Helvetica, sans-serif;
}

/* Style the header */
header {
  background-color: #666;
  padding: 30px;
  text-align: center;
  font-size: 35px;
  color: white;
}

/* Create two columns/boxes that floats next to each other */
nav {
  float: left;
  width: 20%;
  height: 500px; /* only for demonstration, should be removed */
  background: #ccc;
  padding: 20px;
}

/* Style the list inside the menu */
nav ul {
  list-style-type: none;
  padding: 0;
}

article {
  float: left;
  padding: 20px;
  width: 50%;
  background-color: #f1f1f1;
  height: 500px; /* only for demonstration, should be removed */
}

/* Clear floats after the columns */
section::after {
  content: "";
  display: table;
  clear: both;
}

.heading {
  background-color: #d6eaf8;
  text-align: left;
  color: black
  font-size: 15px;
  padding: 20px;
}

.sub-heading {
  background-color: #EBF5FB  ;
  text-align: middle;
  color: #505050;
  font-size: 15px;
  padding: 20px;
}

/* Responsive layout - makes the two columns/boxes stack on top of each other instead of next to each other, on small screens */
@media (max-width: 600px) {
  nav, article {
    width: 100%;
    height: auto;
  }
}
</style>
</head>
<body>


<section>
  <nav>
    <ul>
      <img src="Israt.jpg" alt="Israt Nisa" width=80% height=auto>
      <li> </li>
      <li><a href = "mailto: isratnisa295@gmail.com">My Email</a><li>	
      <li><a href = "https://scholar.google.com/citations?hl=en&user=dfsJMOYAAAAJ">Google Scholar</a></li>
      <li><a href = "https://www.linkedin.com/in/israt-nisa-30686883/">LinkedIn</a><li>
      <li>  </li>
    </ul>
  </nav>
  
  <article>
    <h1>About me</h1>
    <p>I am an applied scientist at AWS AI . I work on scalable Graph Neural Network models for machine learning algorithm (Deep Graph Library - DGL).</p> 
    <p> I was a postdoctoral scholar at Berkeley Lab and have worked on exascale solution for microbiome analysis and dynamic runtime fusion of parallel operators. I did my PhD at the Ohio State University in CSE under the supervision of Prof. P.(Saday) Sadayappan and a core supporter of the very best Buckeye football team. My research specialty lies in high-performance computing - primarily optimizing bandwidth-bound, load imbalanced sparse kernels for massively parallel architectures like GPUs and multi-core CPUs. I earned my undergrad in Computer Science & Engineering from the University of Dhaka, Dhaka, Bangladesh.</p>
  </article>
</section>

<br>

<section>
  <div class="heading">
    <h1 style="text-align: center"; >Projects</h1>
    <div class="sub-heading">
      <h3> DGL </h3>
      <p> Fast and memory-efficient message passing primitives for training Graph Neural Networks. Scale to giant graphs via multi-GPU acceleration and distributed training infrastructure. </p>
       <a href="https://github.com/dmlc/dgl">Github repository</a>  
    </div>
    <div class="sub-heading">
    	<h3> Optimization of Sparse N-D Tensor Kernels </h3>
    	<p> Tensors are used to represent high dimensional data. For
example, the attributes of an email conversation (subject,
author and time) can be represented by the use of a tensor
or a 3-way array. Real world sparse tensors are extremely large, often follow power-law distribution, and are extremely sparse. Canonical Polyadic Decomposition (CPD) is one of the most common tensor factorization techniques, applicable to both dense and sparse tensors, and MTTKRP (multiplying a sparse matricized tensor by Khatri-Rao product), a key kernel, is a common bottleneck for CPD. We propose multiple new data structures to address 1) The fundamental difference between the parallel structure of threads in GPUs versus multicore CPUs, necessitating attention to load-balancing at both levels (between warps in a thread block
and thread blocks in a grid);  the diversity of the nonzero
distribution patterns in a sparse tensor, with different representations being beneficial for ultra-sparse versus moderately
sparse regions of a tensor.  </p>
      <li> <a href="https://github.com/isratnisa/B-CSF">Github repository</a> </li>
      <li> <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=dfsJMOYAAAAJ&sortby=pubdate&alert_preview_top_rm=2&citation_for_view=dfsJMOYAAAAJ:WF5omc3nYNoC">Paper on SC'2019</a> </li>
      <li> <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=dfsJMOYAAAAJ&sortby=pubdate&alert_preview_top_rm=2&citation_for_view=dfsJMOYAAAAJ:LkGwnXOMwfcC">Paper on IPDPS'2019</a> </li>
    </div>
    <div class="sub-heading">
	<h3> Optimization of Sparse 2D Matrix Kernels </h3>
      <p> Sparse Matrix-Vector (SpMV), Sparse Matrix-Multivector (SpMM), Sparse Matrix-Dense Matrix (SDDMM) products are key kernels for computational science and data science. While GPUs offer significantly higher peak performance and memory bandwidth than multicore CPUs, achieving high performance on sparse computations on GPUs is very challenging. We present an in-depth analysis and develop a new sparse-matrix representation and computation approach suited to achieving high data-movement efficiency and effective GPU parallelization. </p>
       <li> <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=dfsJMOYAAAAJ&sortby=pubdate&alert_preview_top_rm=2&citation_for_view=dfsJMOYAAAAJ:Y0pCki6q_DkC">Paper on PPoPP'2019</a> </li>
       <li> <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=dfsJMOYAAAAJ&sortby=pubdate&alert_preview_top_rm=2&citation_for_view=dfsJMOYAAAAJ:Tyk-4Ss8FVUC">Paper on HiPC'2018</a>  </li>   
       <li> More reference papers can be found in the publication section </li>
    </div>
  </div>
</section>
  

</body>
</html>


